{
 "cells": [
  {
   "cell_type": "raw",
   "id": "191f38c1",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–Ω–∏–≥ –ø–æ –ø—Ä–æ–¥–∞–∂–∞–º\n",
    "\n",
    "–≠—Ç–æ—Ç –±–ª–æ–∫–Ω–æ—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç FB2 —Ñ–∞–π–ª—ã –∫–Ω–∏–≥ –ø–æ –ø—Ä–æ–¥–∞–∂–∞–º –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏ –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –∏–∑ –Ω–∏—Ö —Ç–µ—Ö–Ω–∏–∫–∏ –ø—Ä–æ–¥–∞–∂, –¥–∏–∞–ª–æ–≥–∏ –∏ –ø—Ä–∏–º–µ—Ä—ã.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88742cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ò–º–ø–æ—Ä—Ç—ã –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "import csv\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b96447",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceBasedProcessor:\n",
    "    \"\"\"–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ü–µ–ª—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∏ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã—Ö —Ñ—Ä–∞–∑\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –≥–æ—Ç–æ–≤—ã—Ö —Ç–µ—Ö–Ω–∏–∫ –∏ –¥–∏–∞–ª–æ–≥–æ–≤\n",
    "        self.technique_patterns = {\n",
    "            '–≤—ã—è–≤–ª–µ–Ω–∏–µ_–ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–µ–π': [\n",
    "                r'[–ê-–Ø–Å][^.!?]*(?:—á—Ç–æ|–∫–∞–∫|–ø–æ—á–µ–º—É|–∫–æ–≥–¥–∞|–≥–¥–µ|–∫–∞–∫–æ–π|–∫–∞–∫–∞—è|–∫–∞–∫–∏–µ)[^.!?]*\\?',\n",
    "                r'[–ê-–Ø–Å][^.!?]*(?:—Ä–∞—Å—Å–∫–∞–∂–∏—Ç–µ|–æ–±—ä—è—Å–Ω–∏—Ç–µ|–æ–ø–∏—à–∏—Ç–µ|–ø–æ–¥–µ–ª–∏—Ç–µ—Å—å)[^.!?]*[.!?]',\n",
    "                r'[–ê-–Ø–Å][^.!?]*(?:–≤–∞–∂–Ω–æ|–∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç|–≤–æ–ª–Ω—É–µ—Ç|–±–µ—Å–ø–æ–∫–æ–∏—Ç)[^.!?]*\\?',\n",
    "                r'[–ê-–Ø–Å][^.!?]*(?:–ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç|–Ω—É–∂–¥|—Ç—Ä–µ–±–æ–≤–∞–Ω)[^.!?]*[.!?]',\n",
    "                r'[–ê-–Ø–Å][^.!?]*–°–ü–ò–ù[^.!?]*[.!?]'\n",
    "            ],\n",
    "            '–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è_–æ–±—ä–µ–∫—Ç–æ–≤': [\n",
    "                r'[–ê-–Ø–Å][^.!?]*(?:—ç—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç —á—Ç–æ|–±–ª–∞–≥–æ–¥–∞—Ä—è —ç—Ç–æ–º—É|–≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ)[^.!?]*[.!?]',\n",
    "                r'[–ê-–Ø–Å][^.!?]*(?:–ø—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ —Å–µ–±–µ|–ø–æ—Å–º–æ—Ç—Ä–∏—Ç–µ|–æ–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ)[^.!?]*[.!?]',\n",
    "                r'[–ê-–Ø–Å][^.!?]*(?:–≤—ã–≥–æ–¥–∞|–ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ|–ø–æ–ª—å–∑–∞)[^.!?]*[.!?]',\n",
    "                r'[–ê-–Ø–Å][^.!?]*(?:–∫–≤–∞—Ä—Ç–∏—Ä–∞|–¥–æ–º|–æ–±—ä–µ–∫—Ç)[^.!?]*(?:—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫|–æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç)[^.!?]*[.!?]'\n",
    "            ],\n",
    "            '—Ä–∞–±–æ—Ç–∞_—Å_–≤–æ–∑—Ä–∞–∂–µ–Ω–∏—è–º–∏': [\n",
    "                r'[–ê-–Ø–Å][^.!?]*(?:–ø–æ–Ω–∏–º–∞—é|—Å–æ–≥–ª–∞—Å–µ–Ω)[^.!?]*(?:–Ω–æ|–æ–¥–Ω–∞–∫–æ|—Ç–µ–º –Ω–µ –º–µ–Ω–µ–µ)[^.!?]*[.!?]',\n",
    "                r'[–ê-–Ø–Å][^.!?]*(?:–≤–æ–∑—Ä–∞–∂–µ–Ω–∏–µ|—Å–æ–º–Ω–µ–Ω–∏–µ)[^.!?]*[.!?]',\n",
    "                r'[–ê-–Ø–Å][^.!?]*(?:–¥–∞ –Ω–æ –¥–∞|–±—É–º–µ—Ä–∞–Ω–≥)[^.!?]*[.!?]',\n",
    "                r'[–ê-–Ø–Å][^.!?]*–¥–æ—Ä–æ–≥–æ[^.!?]*[.!?]'\n",
    "            ],\n",
    "            '—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ_–¥–æ–≤–µ—Ä–∏—è': [\n",
    "                r'[–ê-–Ø–Å][^.!?]*(?:–æ–ø—ã—Ç –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç|–ø—Ä–∞–∫—Ç–∏–∫–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç)[^.!?]*[.!?]',\n",
    "                r'[–ê-–Ø–Å][^.!?]*(?:–¥—Ä—É–≥–∏–µ –∫–ª–∏–µ–Ω—Ç—ã|–Ω–∞—à–∏ –∫–ª–∏–µ–Ω—Ç—ã)[^.!?]*[.!?]',\n",
    "                r'[–ê-–Ø–Å][^.!?]*(?:–≥–∞—Ä–∞–Ω—Ç–∏—Ä—É—é|–æ–±–µ—â–∞—é|—Ä—É—á–∞—é—Å—å)[^.!?]*[.!?]',\n",
    "                r'[–ê-–Ø–Å][^.!?]*(?:–¥–æ–≤–µ—Ä–∏–µ|–Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å)[^.!?]*[.!?]'\n",
    "            ],\n",
    "            '–∑–∞–∫—Ä—ã—Ç–∏–µ_—Å–¥–µ–ª–∫–∏': [\n",
    "                r'[–ê-–Ø–Å][^.!?]*(?:–≥–æ—Ç–æ–≤—ã|—Å–æ–≥–ª–∞—Å–Ω—ã)[^.!?]*(?:–ø–æ–¥–ø–∏—Å–∞—Ç—å|–æ—Ñ–æ—Ä–º–∏—Ç—å|–∫—É–ø–∏—Ç—å)[^.!?]*\\?',\n",
    "                r'[–ê-–Ø–Å][^.!?]*(?:–∫–æ–≥–¥–∞ —É–¥–æ–±–Ω–æ|–∫–æ–≥–¥–∞ –≤–∞–º –ø–æ–¥—Ö–æ–¥–∏—Ç)[^.!?]*\\?',\n",
    "                r'[–ê-–Ø–Å][^.!?]*(?:–≤—ã–±–∏—Ä–∞–µ—Ç–µ|–ø—Ä–∏–Ω–∏–º–∞–µ—Ç–µ —Ä–µ—à–µ–Ω–∏–µ)[^.!?]*\\?',\n",
    "                r'[–ê-–Ø–Å][^.!?]*(?:–ø–µ—Ä–≤—ã–π –≤–∞—Ä–∏–∞–Ω—Ç –∏–ª–∏ –≤—Ç–æ—Ä–æ–π)[^.!?]*\\?'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥–∏–∞–ª–æ–≥–æ–≤\n",
    "        self.dialog_patterns = [\n",
    "            r'(?:–ú–µ–Ω–µ–¥–∂–µ—Ä|–ü—Ä–æ–¥–∞–≤–µ—Ü|–†–∏–µ–ª—Ç–æ—Ä|–ê–≥–µ–Ω—Ç):\\s*([^.!?]+[.!?])',\n",
    "            r'(?:–ö–ª–∏–µ–Ω—Ç|–ü–æ–∫—É–ø–∞—Ç–µ–ª—å|–ó–∞–∫–∞–∑—á–∏–∫):\\s*([^.!?]+[.!?])',\n",
    "            r'‚Äî\\s*([–ê-–Ø–Å][^.!?]+[.!?])',\n",
    "            r'¬´([^¬ª]+)¬ª[^–ê-–Ø–Å]*–≥–æ–≤–æ—Ä–∏—Ç'\n",
    "        ]\n",
    "        \n",
    "        # –ì–æ—Ç–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã —Ç–µ—Ö–Ω–∏–∫ (–∏–∑ —Ç–µ–æ—Ä–∏–∏ –ø—Ä–æ–¥–∞–∂)\n",
    "        self.theory_examples = {\n",
    "            '–≤—ã—è–≤–ª–µ–Ω–∏–µ_–ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–µ–π': [\n",
    "                \"–ß—Ç–æ –¥–ª—è –≤–∞—Å –≤–∞–∂–Ω–æ –ø—Ä–∏ –≤—ã–±–æ—Ä–µ –∫–≤–∞—Ä—Ç–∏—Ä—ã?\",\n",
    "                \"–†–∞—Å—Å–∫–∞–∂–∏—Ç–µ –æ –≤–∞—à–∏—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è—Ö –∫ –∂–∏–ª—å—é.\",\n",
    "                \"–ö–∞–∫–∏–µ —Ñ–∞–∫—Ç–æ—Ä—ã –≤–ª–∏—è—é—Ç –Ω–∞ –≤–∞—à–µ —Ä–µ—à–µ–Ω–∏–µ?\",\n",
    "                \"–ß—Ç–æ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –≤ –∏–¥–µ–∞–ª—å–Ω–æ–π –∫–≤–∞—Ä—Ç–∏—Ä–µ?\",\n",
    "                \"–ö–∞–∫ –≤—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç–µ —Å–≤–æ–π –Ω–æ–≤—ã–π –¥–æ–º?\"\n",
    "            ],\n",
    "            '–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è_–æ–±—ä–µ–∫—Ç–æ–≤': [\n",
    "                \"–≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –≤—ã —Å—ç–∫–æ–Ω–æ–º–∏—Ç–µ —á–∞—Å –≤—Ä–µ–º–µ–Ω–∏ –∫–∞–∂–¥—ã–π –¥–µ–Ω—å.\",\n",
    "                \"–ë–ª–∞–≥–æ–¥–∞—Ä—è —ç—Ç–æ–π –ø–ª–∞–Ω–∏—Ä–æ–≤–∫–µ –≤–∞—à–∞ —Å–µ–º—å—è –±—É–¥–µ—Ç —á—É–≤—Å—Ç–≤–æ–≤–∞—Ç—å —Å–µ–±—è –∫–æ–º—Ñ–æ—Ä—Ç–Ω–æ.\",\n",
    "                \"–ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ, –∫–∞–∫ –∑–¥–æ—Ä–æ–≤–æ –±—É–¥–µ—Ç –≤—Å—Ç—Ä–µ—á–∞—Ç—å —Ä–∞—Å—Å–≤–µ—Ç—ã –Ω–∞ —ç—Ç–æ–º –±–∞–ª–∫–æ–Ω–µ.\",\n",
    "                \"–≠—Ç–∞ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å –¥–∞–µ—Ç –≤–∞–º –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ –ø–µ—Ä–µ–¥ —Å–æ—Å–µ–¥—è–º–∏.\"\n",
    "            ],\n",
    "            '—Ä–∞–±–æ—Ç–∞_—Å_–≤–æ–∑—Ä–∞–∂–µ–Ω–∏—è–º–∏': [\n",
    "                \"–ü–æ–Ω–∏–º–∞—é –≤–∞—à–∏ —Å–æ–º–Ω–µ–Ω–∏—è, –¥–∞–≤–∞–π—Ç–µ —Ä–∞–∑–±–µ—Ä–µ–º –¥–µ—Ç–∞–ª–∏.\",\n",
    "                \"–°–æ–≥–ª–∞—Å–µ–Ω, —Ü–µ–Ω–∞ –≤–∞–∂–Ω–∞, –Ω–æ –ø–æ—Å–º–æ—Ç—Ä–∏—Ç–µ –Ω–∞ –≤—ã–≥–æ–¥—ã.\",\n",
    "                \"–ú–Ω–æ–≥–∏–µ –∫–ª–∏–µ–Ω—Ç—ã —Å–Ω–∞—á–∞–ª–∞ —Ç–∞–∫ –¥—É–º–∞—é—Ç, –Ω–æ –ø–æ—Ç–æ–º –ø–æ–Ω–∏–º–∞—é—Ç.\",\n",
    "                \"–≠—Ç–æ –≤–∞–∂–Ω—ã–π –≤–æ–ø—Ä–æ—Å, —Å–ø–∞—Å–∏–±–æ —á—Ç–æ –ø–æ–¥–Ω—è–ª–∏ –µ–≥–æ.\"\n",
    "            ],\n",
    "            '—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ_–¥–æ–≤–µ—Ä–∏—è': [\n",
    "                \"–ú–æ–π –æ–ø—ã—Ç –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —ç—Ç–æ –ª—É—á—à–µ–µ —Ä–µ—à–µ–Ω–∏–µ.\",\n",
    "                \"–î—Ä—É–≥–∏–µ –∫–ª–∏–µ–Ω—Ç—ã –æ—Å—Ç–∞–ª–∏—Å—å –æ—á–µ–Ω—å –¥–æ–≤–æ–ª—å–Ω—ã.\",\n",
    "                \"–ì–∞—Ä–∞–Ω—Ç–∏—Ä—É—é, —á—Ç–æ –≤—ã –Ω–µ –ø–æ–∂–∞–ª–µ–µ—Ç–µ –æ –≤—ã–±–æ—Ä–µ.\",\n",
    "                \"–ù–∞—à–∞ –∫–æ–º–ø–∞–Ω–∏—è —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ —Ä—ã–Ω–∫–µ 15 –ª–µ—Ç.\"\n",
    "            ],\n",
    "            '–∑–∞–∫—Ä—ã—Ç–∏–µ_—Å–¥–µ–ª–∫–∏': [\n",
    "                \"–ö–æ–≥–¥–∞ –≤–∞–º —É–¥–æ–±–Ω–æ –ø–æ–¥–ø–∏—Å–∞—Ç—å –¥–æ–≥–æ–≤–æ—Ä?\",\n",
    "                \"–ì–æ—Ç–æ–≤—ã –æ—Ñ–æ—Ä–º–∏—Ç—å –±—Ä–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–µ–≥–æ–¥–Ω—è?\",\n",
    "                \"–ß—Ç–æ –≤—ã–±–∏—Ä–∞–µ—Ç–µ - –ø–µ—Ä–≤—ã–π –≤–∞—Ä–∏–∞–Ω—Ç –∏–ª–∏ –≤—Ç–æ—Ä–æ–π?\",\n",
    "                \"–ö–æ–≥–¥–∞ –ø–ª–∞–Ω–∏—Ä—É–µ—Ç–µ –≤—ä–µ–∑–∂–∞—Ç—å?\"\n",
    "            ]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9f54e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ú–µ—Ç–æ–¥—ã –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ FB2\n",
    "def extract_text_clean(self, file_path):\n",
    "    \"\"\"–ß–∏—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞\"\"\"\n",
    "    \n",
    "    if file_path.suffix.lower() == '.fb2':\n",
    "        return self.extract_fb2_sentences(file_path)\n",
    "    else:\n",
    "        return self.extract_txt_sentences(file_path)\n",
    "\n",
    "def extract_fb2_sentences(self, fb2_path):\n",
    "    \"\"\"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ FB2 —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π\"\"\"\n",
    "    \n",
    "    methods = [\n",
    "        ('xml_parse', self._xml_extract),\n",
    "        ('regex_clean', self._regex_extract),\n",
    "        ('binary_force', self._binary_extract)\n",
    "    ]\n",
    "    \n",
    "    for method_name, method_func in methods:\n",
    "        try:\n",
    "            text = method_func(fb2_path)\n",
    "            if text and len(text) > 1000:\n",
    "                return self.clean_and_structure_text(text)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "def _xml_extract(self, fb2_path):\n",
    "    \"\"\"XML –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ\"\"\"\n",
    "    tree = ET.parse(fb2_path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    paragraphs = []\n",
    "    for elem in root.iter():\n",
    "        if elem.tag and elem.tag.endswith('p') and elem.text:\n",
    "            text = elem.text.strip()\n",
    "            if len(text) > 10:\n",
    "                paragraphs.append(text)\n",
    "    \n",
    "    return '\\n\\n'.join(paragraphs)\n",
    "\n",
    "def _regex_extract(self, fb2_path):\n",
    "    \"\"\"Regex –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å –ø–æ–ø—ã—Ç–∫–æ–π —Ä–∞–∑–Ω—ã—Ö –∫–æ–¥–∏—Ä–æ–≤–æ–∫\"\"\"\n",
    "    for encoding in ['utf-8', 'cp1251', 'koi8-r']:\n",
    "        try:\n",
    "            with open(fb2_path, 'r', encoding=encoding) as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            # –ë–æ–ª–µ–µ —É–º–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞\n",
    "            clean_text = re.sub(r'<[^>]*>', '\\n', content)\n",
    "            clean_text = re.sub(r'\\n\\s*\\n', '\\n\\n', clean_text)\n",
    "            \n",
    "            if len(clean_text) > 2000:\n",
    "                return clean_text\n",
    "        except:\n",
    "            continue\n",
    "    return \"\"\n",
    "\n",
    "def _binary_extract(self, fb2_path):\n",
    "    \"\"\"–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ\"\"\"\n",
    "    with open(fb2_path, 'rb') as f:\n",
    "        raw_data = f.read()\n",
    "    \n",
    "    for encoding in ['utf-8', 'cp1251']:\n",
    "        try:\n",
    "            content = raw_data.decode(encoding, errors='ignore')\n",
    "            clean_text = re.sub(r'<[^>]*>', '\\n', content)\n",
    "            if len(clean_text) > 2000:\n",
    "                return clean_text\n",
    "        except:\n",
    "            continue\n",
    "    return \"\"\n",
    "\n",
    "def extract_txt_sentences(self, txt_path):\n",
    "    \"\"\"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ TXT —Å —Ä–∞–∑–Ω—ã–º–∏ –∫–æ–¥–∏—Ä–æ–≤–∫–∞–º–∏\"\"\"\n",
    "    for encoding in ['utf-8', 'cp1251', 'koi8-r', 'windows-1252']:\n",
    "        try:\n",
    "            with open(txt_path, 'r', encoding=encoding) as f:\n",
    "                text = f.read()\n",
    "            if len(text) > 500:\n",
    "                return self.clean_and_structure_text(text)\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ {txt_path.name}: {e}\")\n",
    "            continue\n",
    "    return \"\"\n",
    "\n",
    "def process_multiple_formats(self, file_path):\n",
    "    \"\"\"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ —Ñ–∞–π–ª–æ–≤\"\"\"\n",
    "    file_ext = file_path.suffix.lower()\n",
    "    \n",
    "    if file_ext == '.fb2':\n",
    "        return self.extract_fb2_sentences(file_path)\n",
    "    elif file_ext == '.txt':\n",
    "        return self.extract_txt_sentences(file_path)\n",
    "    elif file_ext in ['.doc', '.docx']:\n",
    "        return self.extract_doc_text(file_path)\n",
    "    elif file_ext == '.pdf':\n",
    "        return self.extract_pdf_text(file_path)\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç: {file_ext}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_doc_text(self, doc_path):\n",
    "    \"\"\"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ DOC/DOCX —Ñ–∞–π–ª–æ–≤\"\"\"\n",
    "    try:\n",
    "        from docx import Document \n",
    "        doc = Document(doc_path)\n",
    "        text = '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n",
    "        return self.clean_and_structure_text(text) if len(text) > 500 else \"\"\n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ python-docx –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å .docx —Ñ–∞–π–ª–∞–º–∏\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ {doc_path.name}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_pdf_text(self, pdf_path):\n",
    "    \"\"\"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF —Ñ–∞–π–ª–æ–≤\"\"\"\n",
    "    try:\n",
    "        import PyPDF2\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "        return self.clean_and_structure_text(text) if len(text) > 500 else \"\"\n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ PyPDF2 –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å .pdf —Ñ–∞–π–ª–∞–º–∏\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ {pdf_path.name}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–æ–¥—ã –∫ –∫–ª–∞—Å—Å—É\n",
    "SentenceBasedProcessor.extract_text_clean = extract_text_clean\n",
    "SentenceBasedProcessor.extract_fb2_sentences = extract_fb2_sentences\n",
    "SentenceBasedProcessor.extract_txt_sentences = extract_txt_sentences\n",
    "SentenceBasedProcessor.process_multiple_formats = process_multiple_formats\n",
    "SentenceBasedProcessor.extract_doc_text = extract_doc_text\n",
    "SentenceBasedProcessor.extract_pdf_text = extract_pdf_text\n",
    "SentenceBasedProcessor._xml_extract = _xml_extract\n",
    "SentenceBasedProcessor._regex_extract = _regex_extract\n",
    "SentenceBasedProcessor._binary_extract = _binary_extract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a8e4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∫ –∫–ª–∞—Å—Å—É\n",
    "def clean_and_structure_text(self, text):\n",
    "    \"\"\"–û—á–∏—Å—Ç–∫–∞ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞\"\"\"\n",
    "    \n",
    "    # –£–¥–∞–ª—è–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –º—É—Å–æ—Ä\n",
    "    text = re.sub(r'(?:ISBN|–ë–ë–ö|–£–î–ö)[^\\n]*', '', text)\n",
    "    text = re.sub(r'http[^\\s]*', '', text)\n",
    "    text = re.sub(r'\\d{4}-\\d{4}-\\d{4}-\\d{4}', '', text)  # –Ω–æ–º–µ—Ä–∞\n",
    "    \n",
    "    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–æ–±–µ–ª—ã\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "    \n",
    "    # –£–±–∏—Ä–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å—Ç—Ä–æ–∫–∏ (–º—É—Å–æ—Ä)\n",
    "    lines = text.split('\\n')\n",
    "    clean_lines = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if (len(line) > 15 and \n",
    "            not line.isdigit() and\n",
    "            not re.match(r'^[A-Z\\s]{5,}$', line)):  # –Ω–µ —Ç–æ–ª—å–∫–æ –∑–∞–≥–ª–∞–≤–Ω—ã–µ\n",
    "            clean_lines.append(line)\n",
    "    \n",
    "    return '\\n'.join(clean_lines)\n",
    "\n",
    "def find_complete_sentences(self, text, book_name):\n",
    "    \"\"\"–ù–∞—Ö–æ–¥–∏—Ç –¶–ï–õ–´–ï –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å —Ç–µ—Ö–Ω–∏–∫–∞–º–∏\"\"\"\n",
    "    \n",
    "    found_techniques = []\n",
    "    \n",
    "    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è\n",
    "    sentences = re.split(r'[.!?]+\\s+', text)\n",
    "    \n",
    "    for category, patterns in self.technique_patterns.items():\n",
    "        category_sentences = []\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            for sentence in sentences:\n",
    "                sentence = sentence.strip()\n",
    "                if len(sentence) < 10 or len(sentence) > 300:\n",
    "                    continue\n",
    "                \n",
    "                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—É\n",
    "                if re.search(pattern, sentence, re.IGNORECASE):\n",
    "                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞\n",
    "                    if self.is_quality_sentence(sentence, category):\n",
    "                        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–Ω–∞–∫ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è\n",
    "                        if not sentence[-1] in '.!?':\n",
    "                            sentence += '.'\n",
    "                        \n",
    "                        category_sentences.append({\n",
    "                            'book': book_name,\n",
    "                            'category': category,\n",
    "                            'technique': sentence,\n",
    "                            'source': 'extracted',\n",
    "                            'quality': self.rate_sentence_quality(sentence, category)\n",
    "                        })\n",
    "        \n",
    "        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –∏ –±–µ—Ä–µ–º –ª—É—á—à–∏–µ\n",
    "        category_sentences.sort(key=lambda x: x['quality'], reverse=True)\n",
    "        found_techniques.extend(category_sentences[:10])  # –¢–æ–ø-10 –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—é\n",
    "    \n",
    "    return found_techniques\n",
    "\n",
    "def find_dialogs(self, text, book_name):\n",
    "    \"\"\"–ù–∞—Ö–æ–¥–∏—Ç –≥–æ—Ç–æ–≤—ã–µ –¥–∏–∞–ª–æ–≥–∏\"\"\"\n",
    "    \n",
    "    dialogs = []\n",
    "    \n",
    "    for pattern in self.dialog_patterns:\n",
    "        matches = re.finditer(pattern, text, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            dialog_text = match.group(1).strip()\n",
    "            if self.is_quality_dialog(dialog_text):\n",
    "                dialogs.append({\n",
    "                    'book': book_name,\n",
    "                    'category': '–¥–∏–∞–ª–æ–≥_–ø—Ä–∏–º–µ—Ä',\n",
    "                    'technique': dialog_text,\n",
    "                    'source': 'dialog',\n",
    "                    'quality': 9\n",
    "                })\n",
    "    \n",
    "    return dialogs[:15]\n",
    "\n",
    "def is_quality_sentence(self, sentence, category):\n",
    "    \"\"\"–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è\"\"\"\n",
    "    \n",
    "    sentence_lower = sentence.lower()\n",
    "    \n",
    "    # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏\n",
    "    if any(junk in sentence_lower for junk in ['isbn', '–±–±–∫', '—É–¥–∫', 'http']):\n",
    "        return False\n",
    "    \n",
    "    # –î–æ–ª–∂–Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å–ª–æ–≤–∞\n",
    "    relevant_words = {\n",
    "        '–≤—ã—è–≤–ª–µ–Ω–∏–µ_–ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–µ–π': ['–∫–ª–∏–µ–Ω—Ç', '–≤–æ–ø—Ä–æ—Å', '–Ω—É–∂–Ω–æ', '–≤–∞–∂–Ω–æ', '—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ'],\n",
    "        '–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è_–æ–±—ä–µ–∫—Ç–æ–≤': ['–∫–≤–∞—Ä—Ç–∏—Ä–∞', '–¥–æ–º', '–≤—ã–≥–æ–¥–∞', '–ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ', '–æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å'],\n",
    "        '—Ä–∞–±–æ—Ç–∞_—Å_–≤–æ–∑—Ä–∞–∂–µ–Ω–∏—è–º–∏': ['–≤–æ–∑—Ä–∞–∂–µ–Ω–∏–µ', '—Å–æ–º–Ω–µ–Ω–∏–µ', '–ø–æ–Ω–∏–º–∞—é', '—Å–æ–≥–ª–∞—Å–µ–Ω'],\n",
    "        '—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ_–¥–æ–≤–µ—Ä–∏—è': ['–¥–æ–≤–µ—Ä–∏–µ', '–æ–ø—ã—Ç', '–∫–ª–∏–µ–Ω—Ç—ã', '–≥–∞—Ä–∞–Ω—Ç–∏—è'],\n",
    "        '–∑–∞–∫—Ä—ã—Ç–∏–µ_—Å–¥–µ–ª–∫–∏': ['–≥–æ—Ç–æ–≤—ã', '—Ä–µ—à–µ–Ω–∏–µ', '–≤—ã–±–æ—Ä', '–ø–æ–∫—É–ø–∫–∞', '–¥–æ–≥–æ–≤–æ—Ä']\n",
    "    }\n",
    "    \n",
    "    category_words = relevant_words.get(category, [])\n",
    "    return any(word in sentence_lower for word in category_words)\n",
    "\n",
    "def is_quality_dialog(self, dialog_text):\n",
    "    \"\"\"–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∏–∞–ª–æ–≥–∞\"\"\"\n",
    "    return (len(dialog_text) > 10 and \n",
    "            len(dialog_text) < 200 and\n",
    "            any(word in dialog_text.lower() for word in ['–∫–ª–∏–µ–Ω—Ç', '–∫–≤–∞—Ä—Ç–∏—Ä', '–ø–æ–∫—É–ø', '–ø—Ä–æ–¥–∞–∂']))\n",
    "\n",
    "def rate_sentence_quality(self, sentence, category):\n",
    "    \"\"\"–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è\"\"\"\n",
    "    score = 5  # –±–∞–∑–æ–≤—ã–π\n",
    "    \n",
    "    # –ë–æ–Ω—É—Å—ã\n",
    "    if '?' in sentence:\n",
    "        score += 2\n",
    "    if any(word in sentence.lower() for word in ['–∫–ª–∏–µ–Ω—Ç', '–ø–æ–∫—É–ø–∞—Ç–µ–ª—å']):\n",
    "        score += 1\n",
    "    if len(sentence.split()) > 5:\n",
    "        score += 1\n",
    "    if sentence[0].isupper():\n",
    "        score += 1\n",
    "    \n",
    "    return min(10, score)\n",
    "\n",
    "def add_theory_examples(self, found_techniques):\n",
    "    \"\"\"–î–æ–±–∞–≤–ª—è–µ—Ç –≥–æ—Ç–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã –∏–∑ —Ç–µ–æ—Ä–∏–∏\"\"\"\n",
    "    \n",
    "    for category, examples in self.theory_examples.items():\n",
    "        for example in examples:\n",
    "            found_techniques.append({\n",
    "                'book': 'theory_base',\n",
    "                'category': '–±–∞–∑–æ–≤–∞—è_—Ç–µ–æ—Ä–∏—è',\n",
    "                'technique': example,\n",
    "                'source': 'theory',\n",
    "                'quality': 10\n",
    "            })\n",
    "    \n",
    "    return found_techniques\n",
    "\n",
    "# –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ –º–µ—Ç–æ–¥—ã –∫ –∫–ª–∞—Å—Å—É\n",
    "SentenceBasedProcessor.clean_and_structure_text = clean_and_structure_text\n",
    "SentenceBasedProcessor.find_complete_sentences = find_complete_sentences\n",
    "SentenceBasedProcessor.find_dialogs = find_dialogs\n",
    "SentenceBasedProcessor.is_quality_sentence = is_quality_sentence\n",
    "SentenceBasedProcessor.is_quality_dialog = is_quality_dialog\n",
    "SentenceBasedProcessor.rate_sentence_quality = rate_sentence_quality\n",
    "SentenceBasedProcessor.add_theory_examples = add_theory_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fdf56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ –∏ –∑–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É\n",
    "processor = SentenceBasedProcessor()\n",
    "\n",
    "# –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ –∫–Ω–∏–≥–∏ (–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã)\n",
    "books_dir = Path(\"books\")\n",
    "all_files = (list(books_dir.glob(\"*.fb2\")) + \n",
    "             list(books_dir.glob(\"*.txt\")) + \n",
    "             list(books_dir.glob(\"*.docx\")) + \n",
    "             list(books_dir.glob(\"*.doc\")) + \n",
    "             list(books_dir.glob(\"*.pdf\")))\n",
    "\n",
    "print(\"‚ú® –û–ë–†–ê–ë–û–¢–ö–ê –ù–ê –û–°–ù–û–í–ï –¶–ï–õ–´–• –ü–†–ï–î–õ–û–ñ–ï–ù–ò–ô\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìÅ –§–∞–π–ª–æ–≤: {len(all_files)}\")\n",
    "print()\n",
    "\n",
    "all_techniques = []\n",
    "book_stats = {}\n",
    "\n",
    "for file_path in all_files:\n",
    "    print(f\"üìñ {file_path.name}\")\n",
    "    \n",
    "    text = processor.process_multiple_formats(file_path)\n",
    "    if text:\n",
    "        # –ò—â–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å —Ç–µ—Ö–Ω–∏–∫–∞–º–∏\n",
    "        sentences = processor.find_complete_sentences(text, file_path.name)\n",
    "        \n",
    "        # –ò—â–µ–º –¥–∏–∞–ª–æ–≥–∏\n",
    "        dialogs = processor.find_dialogs(text, file_path.name)\n",
    "        \n",
    "        book_techniques = sentences + dialogs\n",
    "        all_techniques.extend(book_techniques)\n",
    "        book_stats[file_path.name] = len(book_techniques)\n",
    "        \n",
    "        print(f\"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ: {len(book_techniques)} –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ç–µ—Ö–Ω–∏–∫\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å\")\n",
    "        book_stats[file_path.name] = 0\n",
    "    \n",
    "    print()\n",
    "\n",
    "# –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã\n",
    "all_techniques = processor.add_theory_examples(all_techniques)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üéâ –†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–ë–†–ê–ë–û–¢–ö–ò:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìö –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∫–Ω–∏–≥: {len([b for b, c in book_stats.items() if c > 0])}\")\n",
    "print(f\"‚ú® –í—Å–µ–≥–æ —Ç–µ—Ö–Ω–∏–∫: {len(all_techniques)}\")\n",
    "print(f\"\\nüìñ –¢–µ—Ö–Ω–∏–∫–∏ –ø–æ –∫–Ω–∏–≥–∞–º:\")\n",
    "for book, count in book_stats.items():\n",
    "    if count > 0:\n",
    "        short_name = book[:40] + \"...\" if len(book) > 40 else book\n",
    "        print(f\"  {short_name}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398b6218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ CSV\n",
    "data_dir = Path(\"data\")\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# CSV —Å —á–∏—Ç–∞–µ–º—ã–º–∏ —Ç–µ—Ö–Ω–∏–∫–∞–º–∏\n",
    "csv_path = data_dir / \"quality_techniques.csv\"\n",
    "with open(csv_path, 'w', encoding='utf-8', newline='') as f:\n",
    "    writer = csv.writer(f, quoting=csv.QUOTE_ALL)\n",
    "    writer.writerow(['book', 'category', 'source', 'quality', 'sales_technique'])\n",
    "    for technique in all_techniques:\n",
    "        book = technique['book']\n",
    "        category = technique['category']\n",
    "        source = technique['source']\n",
    "        quality = technique['quality']\n",
    "        text = technique['technique'].replace('\\n', ' ')\n",
    "        writer.writerow([book, category, source, quality, text])\n",
    "\n",
    "print(f\"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤: {csv_path}\")\n",
    "print(f\"üìä –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(all_techniques)}\")\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º DataFrame –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞\n",
    "df = pd.read_csv(csv_path)\n",
    "print(f\"\\nüìã –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö:\")\n",
    "print(df.info())\n",
    "print(f\"\\nüìà –ü–µ—Ä–≤—ã–µ 5 –∑–∞–ø–∏—Å–µ–π:\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d02e9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö —Ç–µ—Ö–Ω–∏–∫\n",
    "\n",
    "# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
    "print(\"üìä –ê–ù–ê–õ–ò–ó –ö–ê–ß–ï–°–¢–í–ê –î–ê–ù–ù–´–•\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"üìö –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\")\n",
    "print(f\"   ‚Ä¢ –í—Å–µ–≥–æ —Ç–µ—Ö–Ω–∏–∫: {len(df)}\")\n",
    "print(f\"   ‚Ä¢ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–Ω–∏–≥: {df['book'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ –ö–∞—Ç–µ–≥–æ—Ä–∏–π: {df['category'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ –ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {df['source'].nunique()}\")\n",
    "\n",
    "print(f\"\\nüèÜ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É:\")\n",
    "quality_counts = df['quality'].value_counts().sort_index()\n",
    "for quality, count in quality_counts.items():\n",
    "    percentage = count / len(df) * 100\n",
    "    print(f\"   ‚Ä¢ –ö–∞—á–µ—Å—Ç–≤–æ {quality}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüìñ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:\")\n",
    "category_counts = df['category'].value_counts()\n",
    "for category, count in category_counts.items():\n",
    "    percentage = count / len(df) * 100\n",
    "    print(f\"   ‚Ä¢ {category}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüìö –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–Ω–∏–≥–∞–º:\")\n",
    "book_counts = df['book'].value_counts()\n",
    "for book, count in book_counts.items():\n",
    "    percentage = count / len(df) * 100\n",
    "    short_name = book[:35] + \"...\" if len(book) > 35 else book\n",
    "    print(f\"   ‚Ä¢ {short_name}: {count} ({percentage:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946964cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('üìä –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö —Ç–µ—Ö–Ω–∏–∫ –ø—Ä–æ–¥–∞–∂', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É\n",
    "ax1 = axes[0, 0]\n",
    "quality_counts.plot(kind='bar', ax=ax1, color='skyblue', edgecolor='navy')\n",
    "ax1.set_title('üèÜ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É')\n",
    "ax1.set_xlabel('–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞')\n",
    "ax1.set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ—Ö–Ω–∏–∫')\n",
    "ax1.tick_params(axis='x', rotation=0)\n",
    "\n",
    "# –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã\n",
    "for i, v in enumerate(quality_counts.values):\n",
    "    ax1.text(i, v + 0.5, str(v), ha='center', va='bottom')\n",
    "\n",
    "# 2. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º\n",
    "ax2 = axes[0, 1]\n",
    "category_counts.plot(kind='bar', ax=ax2, color='lightgreen', edgecolor='darkgreen')\n",
    "ax2.set_title('üìñ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º')\n",
    "ax2.set_xlabel('–ö–∞—Ç–µ–≥–æ—Ä–∏—è')\n",
    "ax2.set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ—Ö–Ω–∏–∫')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã\n",
    "for i, v in enumerate(category_counts.values):\n",
    "    ax2.text(i, v + 0.5, str(v), ha='center', va='bottom')\n",
    "\n",
    "# 3. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–Ω–∏–≥–∞–º (—Ç–æ–ø-7)\n",
    "ax3 = axes[1, 0]\n",
    "book_counts.head(7).plot(kind='bar', ax=ax3, color='coral', edgecolor='darkred')\n",
    "ax3.set_title('üìö –¢–æ–ø-7 –∫–Ω–∏–≥ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Ç–µ—Ö–Ω–∏–∫')\n",
    "ax3.set_xlabel('–ö–Ω–∏–≥–∞')\n",
    "ax3.set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ—Ö–Ω–∏–∫')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# –°–æ–∫—Ä–∞—â–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –∫–Ω–∏–≥ –¥–ª—è –ª—É—á—à–µ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "labels = [label.get_text()[:20] + '...' if len(label.get_text()) > 20 else label.get_text() \n",
    "          for label in ax3.get_xticklabels()]\n",
    "ax3.set_xticklabels(labels)\n",
    "\n",
    "# –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã\n",
    "for i, v in enumerate(book_counts.head(7).values):\n",
    "    ax3.text(i, v + 0.5, str(v), ha='center', va='bottom')\n",
    "\n",
    "# 4. –ö–∞—á–µ—Å—Ç–≤–æ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º\n",
    "ax4 = axes[1, 1]\n",
    "source_quality = df.groupby('source')['quality'].mean()\n",
    "source_quality.plot(kind='bar', ax=ax4, color='plum', edgecolor='purple')\n",
    "ax4.set_title('üìà –°—Ä–µ–¥–Ω–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º')\n",
    "ax4.set_xlabel('–ò—Å—Ç–æ—á–Ω–∏–∫')\n",
    "ax4.set_ylabel('–°—Ä–µ–¥–Ω–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ')\n",
    "ax4.tick_params(axis='x', rotation=0)\n",
    "\n",
    "# –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã\n",
    "for i, v in enumerate(source_quality.values):\n",
    "    ax4.text(i, v + 0.05, f'{v:.1f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ab0749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–µ—Ö–Ω–∏–∫ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º\n",
    "\n",
    "print(\"üîç –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –ü–û –ö–ê–¢–ï–ì–û–†–ò–Ø–ú\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "categories = df['category'].unique()\n",
    "\n",
    "for category in sorted(categories):\n",
    "    category_df = df[df['category'] == category]\n",
    "    print(f\"\\nüìã {category.upper()}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"   ‚Ä¢ –í—Å–µ–≥–æ —Ç–µ—Ö–Ω–∏–∫: {len(category_df)}\")\n",
    "    print(f\"   ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ: {category_df['quality'].mean():.1f}\")\n",
    "    print(f\"   ‚Ä¢ –ú–∏–Ω/–ú–∞–∫—Å –∫–∞—á–µ—Å—Ç–≤–æ: {category_df['quality'].min()}/{category_df['quality'].max()}\")\n",
    "    \n",
    "    # –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏\n",
    "    sources = category_df['source'].value_counts()\n",
    "    print(f\"   ‚Ä¢ –ò—Å—Ç–æ—á–Ω–∏–∫–∏: {dict(sources)}\")\n",
    "    \n",
    "    # –¢–æ–ø-3 —Ç–µ—Ö–Ω–∏–∫–∏ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É\n",
    "    top_techniques = category_df.nlargest(3, 'quality')\n",
    "    print(f\"   ‚Ä¢ –¢–æ–ø-3 —Ç–µ—Ö–Ω–∏–∫–∏:\")\n",
    "    for idx, (_, row) in enumerate(top_techniques.iterrows(), 1):\n",
    "        technique_preview = row['sales_technique'][:80] + \"...\" if len(row['sales_technique']) > 80 else row['sales_technique']\n",
    "        print(f\"     {idx}. [{row['quality']}‚≠ê] {technique_preview}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aeb587e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ê–Ω–∞–ª–∏–∑ –¥–ª–∏–Ω—ã –∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–µ—Ö–Ω–∏–∫\n",
    "\n",
    "# –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–æ–ª–±–µ—Ü —Å –¥–ª–∏–Ω–æ–π —Ç–µ—Ö–Ω–∏–∫\n",
    "df['technique_length'] = df['sales_technique'].str.len()\n",
    "\n",
    "print(\"üìè –ê–ù–ê–õ–ò–ó –î–õ–ò–ù–´ –¢–ï–•–ù–ò–ö\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"   ‚Ä¢ –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞: {df['technique_length'].mean():.0f} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
    "print(f\"   ‚Ä¢ –ú–µ–¥–∏–∞–Ω–Ω–∞—è –¥–ª–∏–Ω–∞: {df['technique_length'].median():.0f} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
    "print(f\"   ‚Ä¢ –ú–∏–Ω/–ú–∞–∫—Å –¥–ª–∏–Ω–∞: {df['technique_length'].min()}/{df['technique_length'].max()}\")\n",
    "\n",
    "# –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –º–µ–∂–¥—É –¥–ª–∏–Ω–æ–π –∏ –∫–∞—á–µ—Å—Ç–≤–æ–º\n",
    "correlation = df['technique_length'].corr(df['quality'])\n",
    "print(f\"   ‚Ä¢ –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –¥–ª–∏–Ω–∞-–∫–∞—á–µ—Å—Ç–≤–æ: {correlation:.3f}\")\n",
    "\n",
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –¥–ª–∏–Ω—ã –æ—Ç –∫–∞—á–µ—Å—Ç–≤–∞\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(df['technique_length'], df['quality'], alpha=0.6, color='steelblue')\n",
    "plt.xlabel('–î–ª–∏–Ω–∞ —Ç–µ—Ö–Ω–∏–∫–∏ (—Å–∏–º–≤–æ–ª—ã)')\n",
    "plt.ylabel('–ö–∞—á–µ—Å—Ç–≤–æ')\n",
    "plt.title('üìä –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç –¥–ª–∏–Ω—ã')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# –î–æ–±–∞–≤–ª—è–µ–º –ª–∏–Ω–∏—é —Ç—Ä–µ–Ω–¥–∞\n",
    "z = np.polyfit(df['technique_length'], df['quality'], 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(df['technique_length'], p(df['technique_length']), \"r--\", alpha=0.8)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "df.boxplot(column='technique_length', by='category', ax=plt.gca())\n",
    "plt.title('üìè –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–ª–∏–Ω—ã –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º')\n",
    "plt.suptitle('')  # –£–±–∏—Ä–∞–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('–î–ª–∏–Ω–∞ —Ç–µ—Ö–Ω–∏–∫–∏ (—Å–∏–º–≤–æ–ª—ã)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2894c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –§–∏–Ω–∞–ª—å–Ω–∞—è —Å–≤–æ–¥–∫–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏\n",
    "\n",
    "print(\"üìã –ò–¢–û–ì–û–í–ê–Ø –°–í–û–î–ö–ê\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º —Å–≤–æ–¥–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É\n",
    "summary_stats = {\n",
    "    'total_techniques': len(df),\n",
    "    'books_processed': df['book'].nunique(),\n",
    "    'categories': df['category'].nunique(),\n",
    "    'avg_quality': df['quality'].mean(),\n",
    "    'high_quality_techniques': len(df[df['quality'] >= 9]),\n",
    "    'avg_technique_length': df['technique_length'].mean(),\n",
    "    'top_category': category_counts.index[0],\n",
    "    'top_book': book_counts.index[0] if len(book_counts) > 0 else 'N/A'\n",
    "}\n",
    "\n",
    "print(f\"‚ú® –û–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏:\")\n",
    "print(f\"   ‚Ä¢ –í—Å–µ–≥–æ —Ç–µ—Ö–Ω–∏–∫: {summary_stats['total_techniques']}\")\n",
    "print(f\"   ‚Ä¢ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∫–Ω–∏–≥: {summary_stats['books_processed']}\")\n",
    "print(f\"   ‚Ä¢ –ö–∞—Ç–µ–≥–æ—Ä–∏–π —Ç–µ—Ö–Ω–∏–∫: {summary_stats['categories']}\")\n",
    "print(f\"   ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ: {summary_stats['avg_quality']:.1f}/10\")\n",
    "print(f\"   ‚Ä¢ –í—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ç–µ—Ö–Ω–∏–∫ (‚â•9): {summary_stats['high_quality_techniques']}\")\n",
    "print(f\"   ‚Ä¢ –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Ç–µ—Ö–Ω–∏–∫–∏: {summary_stats['avg_technique_length']:.0f} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
    "\n",
    "print(f\"\\nüèÜ –õ–∏–¥–µ—Ä—ã:\")\n",
    "print(f\"   ‚Ä¢ –°–∞–º–∞—è –ø–æ–ø—É–ª—è—Ä–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è: {summary_stats['top_category']}\")\n",
    "print(f\"   ‚Ä¢ –°–∞–º–∞—è –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–∞—è –∫–Ω–∏–≥–∞: {summary_stats['top_book'][:40]}...\")\n",
    "\n",
    "print(f\"\\nüí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è:\")\n",
    "low_quality_count = len(df[df['quality'] < 7])\n",
    "if low_quality_count > 0:\n",
    "    print(f\"   ‚Ä¢ –ü–µ—Ä–µ—Å–º–æ—Ç—Ä–µ—Ç—å {low_quality_count} —Ç–µ—Ö–Ω–∏–∫ —Å –∫–∞—á–µ—Å—Ç–≤–æ–º < 7\")\n",
    "\n",
    "category_imbalance = category_counts.max() / category_counts.min()\n",
    "if category_imbalance > 3:\n",
    "    print(f\"   ‚Ä¢ –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (–¥–∏—Å–±–∞–ª–∞–Ω—Å {category_imbalance:.1f}x)\")\n",
    "\n",
    "if df['technique_length'].std() > 100:\n",
    "    print(f\"   ‚Ä¢ –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–ª–∏–Ω—É —Ç–µ—Ö–Ω–∏–∫ (—Ä–∞–∑–±—Ä–æ—Å {df['technique_length'].std():.0f})\")\n",
    "\n",
    "print(f\"\\nüìÅ –°–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:\")\n",
    "print(f\"   ‚Ä¢ {csv_path} - –æ—Å–Ω–æ–≤–Ω–æ–π –¥–∞—Ç–∞—Å–µ—Ç\")\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É\n",
    "summary_path = data_dir / \"processing_summary.json\"\n",
    "import json\n",
    "with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary_stats, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"   ‚Ä¢ {summary_path} - —Å–≤–æ–¥–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\")\n",
    "print(f\"\\n‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cc1300b6",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
